{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5e5283-0ed2-4f6f-96dc-46143db944ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a74b7fe-4679-4eff-a775-a0a8929b7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "para =\"\"\"During my academic journey, I developed a strong interest in Natural Language Processing (NLP), a domain that combines the power of linguistics and machine learning. To build a strong foundation, I extensively worked with the NLTK (Natural Language Toolkit), an essential Python library for text processing and computational linguistics. Using NLTK, I gained hands-on experience with tasks such as tokenization, stemming, lemmatization, POS tagging, and sentiment analysis. I implemented various NLP pipelines for analyzing textual data and built small projects such as chatbot frameworks and automated text summarizers. This practical exposure helped me understand the complexity of human language and the challenges in machine understanding. I am now eager to take this knowledge forward in an advanced research environment like TUM, where I can deepen my expertise and contribute to innovative NLP applications with real-world impact.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47e9381-3481-4e12-806f-45d00c3f6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c43215c-6bae-4f5b-9bb6-91a3e34c275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910207be-1e8b-4946-ac34-69e8e58adaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sent)):\n",
    "    words = nltk.word_tokenize(sent[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sent[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbcb9fe-b368-4c8b-aa1e-3555e9d095ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['During academic journey , I developed strong interest Natural Language Processing ( NLP ) , domain combine power linguistics machine learning .',\n",
       " 'To build strong foundation , I extensively worked NLTK ( Natural Language Toolkit ) , essential Python library text processing computational linguistics .',\n",
       " 'Using NLTK , I gained hands-on experience task tokenization , stemming , lemmatization , POS tagging , sentiment analysis .',\n",
       " 'I implemented various NLP pipeline analyzing textual data built small project chatbot framework automated text summarizers .',\n",
       " 'This practical exposure helped understand complexity human language challenge machine understanding .',\n",
       " 'I eager take knowledge forward advanced research environment like TUM , I deepen expertise contribute innovative NLP application real-world impact .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c35d53-8a4d-44c8-9514-646eae7badf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
